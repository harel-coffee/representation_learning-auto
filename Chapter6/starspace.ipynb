{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StarSpace\n",
    "\n",
    "StarSpace [[1]](#fn1) is an entity embedding approach which uses a similarity function between entities to construct a prediction task for a neural network. It maps objects of different types into a common vector space where they can be compared to each other. StarSpace can learn word, sentence and document level embeddings, ranking, text classification, embedding graphs, image classification, etc. We will follow the official documentation of StarSpace and implement simple text classification.\n",
    "\n",
    "This notebook requires a working SparSpace program which can be built on any modern Linux or Windows machine as described in the building instructions in the [GitHub repository](https://github.com/facebookresearch/StarSpace). Here, we use the Linux toolchain to build the StarSpace executable. If you run this notebook on Windows, you can use either Visual Studio or tools such as [MinGW with MSYS](http://www.mingw.org/) or [Cygwin](https://www.cygwin.com/) to compile StarSpace.\n",
    "\n",
    "-----\n",
    "<span id=\"fn1\"> [1] Ledell Yu Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, and Jason Weston. Starspace: Embed all the things! In Proceedings of the 32nd AAAI Conference on Artificial Intelligence, pages 5569â€“5577, 2018. </span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to ensure that all the required libraries are available. The `-q` parameter is used to suppress long installation reports produced by `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install gensim==3.8.3\n",
    "!pip -q install matplotlib==3.3.3\n",
    "!pip -q install scikit-learn==0.23.2\n",
    "!pip -q install pandas==1.1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clone the source code repository and compile the starspace binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'StarSpace'...\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 873 (delta 0), reused 0 (delta 0), pack-reused 868\u001b[K\n",
      "Receiving objects: 100% (873/873), 3.05 MiB | 5.39 MiB/s, done.\n",
      "Resolving deltas: 100% (567/567), done.\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -g -c src/utils/normalize.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/dict.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -g -c src/utils/args.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/proj.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/parser.cpp -o parser.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/data.cpp -o data.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/model.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/starspace.cpp\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/doc_parser.cpp -o doc_parser.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/doc_data.cpp -o doc_data.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops -I/usr/local/bin/boost_1_63_0/ -g -c src/utils/utils.cpp -o utils.o\n",
      "g++ -pthread -std=gnu++11 -O3 -funroll-loops normalize.o dict.o args.o proj.o parser.o data.o model.o starspace.o doc_parser.o doc_data.o utils.o -I/usr/local/bin/boost_1_63_0/ -g src/main.cpp -o starspace\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "if os.name == 'nt':\n",
    "    print('ERROR: you are running this notebook on a Windows system. Please open the StarSpace Visual Studio solution file (https://github.com/facebookresearch/StarSpace/blob/master/MVS/StarSpace.sln) and build the project.')   \n",
    "    raise StopExecution\n",
    "else:\n",
    "    !git clone git@github.com:facebookresearch/StarSpace.git && cd StarSpace && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The executable is now available as `StarSpace/starspace`. The original bash script (classification_ag_news.sh) for the text classification example is available in the [Starspace GitHub repository](https://github.com/facebookresearch/Starspace/blob/master/examples/classification_ag_news.sh). We reimplement it as a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is based on [Antonio Gulli's corpus (AG)](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) which is a collection of more than 1 million news articles. From this collection, Zhang et al. [[2]](#fn2) constructed a smaller corpus, containing only the four largest news categoriess from the original corpus. Each category (i.e. class value) contains 30,000 training instances and 1,900 testing instances. The total number of training samples is 120000 while 7600 samples are reserved for testing. We download, unpack and inspect the corpus.\n",
    "\n",
    "----\n",
    "<span id=\"fn2\"> [2] Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).</span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['classes.txt', 'test.csv', 'readme.txt', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import requests\n",
    "import os\n",
    "\n",
    "request = requests.get('https://dl.fbaipublicfiles.com/starspace/ag_news_csv.tar.gz')\n",
    "with open(\"data/ag_news_csv.tar.gz\", \"wb\") as file:\n",
    "    file.write(request.content)\n",
    "\n",
    "with tarfile.open('data/ag_news_csv.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall(path='data')\n",
    "print(os.listdir('data/ag_news_csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four classes and each news from the train and test set is classified using the line number of the actual class value. The training data looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  categories\n",
      "0      World\n",
      "1     Sports\n",
      "2   Business\n",
      "3   Sci/Tech\n",
      "   category                          title                           body\n",
      "0         3  Wall St. Bears Claw Back I...  Reuters - Short-sellers, W...\n",
      "1         3  Carlyle Looks Toward Comme...  Reuters - Private investme...\n",
      "2         3  Oil and Economy Cloud Stoc...  Reuters - Soaring crude pr...\n",
      "3         3  Iraq Halts Oil Exports fro...  Reuters - Authorities have...\n",
      "4         3  Oil prices soar to all-tim...  AFP - Tearaway world oil p...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "print(pd.read_csv('data/ag_news_csv/classes.txt', names=['categories']))\n",
    "print(pd.read_csv('data/ag_news_csv/train.csv', names=['category', 'title', 'body']).iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the data into a Pandas DataFrame object and preprocess the text by converting it to lowercase and replacing a number of characters. The category is prefixed with `__label__` as required for the fastText word embedding file format. The transformed data is randomly shuffled and written into a fastText compatible text file. The four news categories are balanced in the train as well as in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File train.csv\n",
      "{'__label__business': 30000,\n",
      " '__label__scitech': 30000,\n",
      " '__label__sports': 30000,\n",
      " '__label__world': 30000}\n",
      "File test.csv\n",
      "{'__label__business': 1900,\n",
      " '__label__scitech': 1900,\n",
      " '__label__sports': 1900,\n",
      " '__label__world': 1900}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "idx2category = {1: '__label__world',2: '__label__sports', 3:'__label__business', 4:'__label__scitech'}\n",
    "\n",
    "def preprocess(df):\n",
    "    df = df.replace({'category': idx2category})\n",
    "    df['text'] = df['title'] + ' ' + df['body']\n",
    "    df = df.drop(labels=['title', 'body'], axis=1)\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    for s, rep in [(\"'\",\" ' \"),\n",
    "                   ('\"',''),\n",
    "                   ('.',' . '),\n",
    "                   ('<br />',' '),\n",
    "                   (',',' , '),\n",
    "                   ('(',' ( '),\n",
    "                   (')',' ) '),\n",
    "                   ('!',' ! '),\n",
    "                   ('?',' ? '),\n",
    "                   (';',' '),\n",
    "                   (':',' '),\n",
    "                   ('\\\\',''),\n",
    "                   ('  ',' ')\n",
    "                  ]:\n",
    "        df['text'] = df['text'].str.replace(s, rep)   \n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    return df\n",
    "\n",
    "for filename in ['data/ag_news_csv/train.csv','data/ag_news_csv/test.csv']:\n",
    "    df = pd.read_csv(filename, names=['category', 'title', 'body'])\n",
    "    df = preprocess(df)\n",
    "    print('File {}'.format(os.path.split(filename)[1]))\n",
    "    pprint(df['category'].value_counts().to_dict())\n",
    "    with open('{}.pp'.format(os.path.splitext(filename)[0]), 'w') as fp:\n",
    "        for row in df.itertuples():\n",
    "            fp.write('{} {}\\n'.format(row.category, row.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run StarSpace on the preprocessed files. The set of parameters is the same as in the example from the StarSpace repository. The `trainMode=0` and `fileFormat='FastText'` combinations defines the mode where the labels are individual words, i.e. the classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 5\n",
      "batchSize: 5\n",
      "thread: 20\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 0\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/ag_news_csv/train.pp\n",
      "Read 5M words\n",
      "Number of words in dictionary:  94698\n",
      "Number of labels in dictionary: 4\n",
      "Loading data from file : data/ag_news_csv/train.pp\n",
      "Total number of examples loaded : 120000\n",
      "Training epoch 0: 0.01 0.002\n",
      "Epoch: 100.0%  lr: 0.008117  loss: 0.035385  eta: <1min   tot: 0h0m0s  (20.0%)0.2%  lr: 0.008833  loss: 0.043099  eta: <1min   tot: 0h0m0s  (12.0%)74.4%  lr: 0.008600  loss: 0.039451  eta: <1min   tot: 0h0m0s  (14.9%)99.7%  lr: 0.008117  loss: 0.035413  eta: <1min   tot: 0h0m0s  (19.9%)\n",
      " ---+++                Epoch    0 Train error : 0.03201538 +++--- â˜ƒ\n",
      "Training epoch 1: 0.008 0.002\n",
      "Epoch: 100.0%  lr: 0.006000  loss: 0.018529  eta: <1min   tot: 0h0m0s  (40.0%)5.4%  lr: 0.006500  loss: 0.019303  eta: <1min   tot: 0h0m0s  (31.1%)64.9%  lr: 0.006317  loss: 0.018866  eta: <1min   tot: 0h0m0s  (33.0%)\n",
      " ---+++                Epoch    1 Train error : 0.01761493 +++--- â˜ƒ\n",
      "Training epoch 2: 0.006 0.002\n",
      "Epoch: 100.0%  lr: 0.004183  loss: 0.014683  eta: <1min   tot: 0h0m1s  (60.0%) lr: 0.005900  loss: 0.012627  eta: <1min   tot: 0h0m0s  (40.9%)14.2%  lr: 0.005783  loss: 0.014844  eta: <1min   tot: 0h0m0s  (42.8%)23.7%  lr: 0.005583  loss: 0.015281  eta: <1min   tot: 0h0m1s  (44.7%)57.0%  lr: 0.004950  loss: 0.015072  eta: <1min   tot: 0h0m1s  (51.4%)\n",
      " ---+++                Epoch    2 Train error : 0.01478347 +++--- â˜ƒ\n",
      "Training epoch 3: 0.004 0.002\n",
      "Epoch: 100.0%  lr: 0.002000  loss: 0.012871  eta: <1min   tot: 0h0m1s  (80.0%)2%  lr: 0.003817  loss: 0.017381  eta: <1min   tot: 0h0m1s  (60.6%)14.2%  lr: 0.003617  loss: 0.012978  eta: <1min   tot: 0h0m1s  (62.8%)23.7%  lr: 0.003433  loss: 0.012063  eta: <1min   tot: 0h0m1s  (64.7%)53.8%  lr: 0.002983  loss: 0.011820  eta: <1min   tot: 0h0m1s  (70.8%)74.4%  lr: 0.002317  loss: 0.012698  eta: <1min   tot: 0h0m1s  (74.9%)\n",
      " ---+++                Epoch    3 Train error : 0.01287099 +++--- â˜ƒ\n",
      "Training epoch 4: 0.002 0.002\n",
      "Epoch: 100.0%  lr: -0.000000  loss: 0.011717  eta: <1min   tot: 0h0m2s  (100.0%)r: 0.001867  loss: 0.014404  eta: <1min   tot: 0h0m1s  (80.9%)15.8%  lr: 0.001467  loss: 0.012105  eta: <1min   tot: 0h0m1s  (83.2%)23.7%  lr: 0.001250  loss: 0.012151  eta: <1min   tot: 0h0m1s  (84.7%)58.6%  lr: 0.000533  loss: 0.011196  eta: <1min   tot: 0h0m1s  (91.7%)69.7%  lr: 0.000183  loss: 0.011304  eta: <1min   tot: 0h0m2s  (93.9%)\n",
      " ---+++                Epoch    4 Train error : 0.01133779 +++--- â˜ƒ\n",
      "Saving model to file : data/ag_news_csv/model\n",
      "Saving model in tsv format : data/ag_news_csv/model.tsv\n"
     ]
    }
   ],
   "source": [
    "!./StarSpace/starspace train \\\n",
    "  -trainFile \"data/ag_news_csv/train.pp\" \\\n",
    "  -model \"data/ag_news_csv/model\" \\\n",
    "  -initRandSd 0.01 \\\n",
    "  -adagrad false \\\n",
    "  -ngrams 1 \\\n",
    "  -lr 0.01 \\\n",
    "  -epoch 5 \\\n",
    "  -thread 20 \\\n",
    "  -dim 10 \\\n",
    "  -negSearchLimit 5 \\\n",
    "  -trainMode 0 \\\n",
    "  -label \"__label__\" \\\n",
    "  -similarity \"dot\" \\\n",
    "  -verbose false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Starspace model embeddsthe input into a common 10-dimensional space (set by the `-dim 10` setting). We load it into a dataframe and inspect it. As shown in the table below, the model embedds everything into a common space: words that are present in documents but also the categories (the last four rows). In this way, we can now compare entities of different kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>-0.001581</td>\n",
       "      <td>-0.055738</td>\n",
       "      <td>-0.001461</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>-0.024389</td>\n",
       "      <td>0.012898</td>\n",
       "      <td>-0.027400</td>\n",
       "      <td>0.030329</td>\n",
       "      <td>-0.078572</td>\n",
       "      <td>-0.081473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>0.041248</td>\n",
       "      <td>0.020253</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>0.004240</td>\n",
       "      <td>-0.013099</td>\n",
       "      <td>0.036625</td>\n",
       "      <td>0.028696</td>\n",
       "      <td>0.005871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>-0.044975</td>\n",
       "      <td>0.007411</td>\n",
       "      <td>-0.001072</td>\n",
       "      <td>-0.001351</td>\n",
       "      <td>0.026816</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.010960</td>\n",
       "      <td>-0.018680</td>\n",
       "      <td>-0.026508</td>\n",
       "      <td>-0.018127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>-0.091575</td>\n",
       "      <td>-0.034052</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>-0.002135</td>\n",
       "      <td>-0.019016</td>\n",
       "      <td>0.052091</td>\n",
       "      <td>-0.035150</td>\n",
       "      <td>-0.017636</td>\n",
       "      <td>-0.067598</td>\n",
       "      <td>0.067879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>0.017022</td>\n",
       "      <td>0.029204</td>\n",
       "      <td>-0.007912</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>-0.014567</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>-0.013684</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94697</th>\n",
       "      <td>maleafter</td>\n",
       "      <td>0.010264</td>\n",
       "      <td>-0.013049</td>\n",
       "      <td>-0.005277</td>\n",
       "      <td>0.017525</td>\n",
       "      <td>-0.015361</td>\n",
       "      <td>0.006922</td>\n",
       "      <td>-0.019601</td>\n",
       "      <td>-0.002084</td>\n",
       "      <td>-0.017456</td>\n",
       "      <td>0.004337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94698</th>\n",
       "      <td>__label__business</td>\n",
       "      <td>-0.216275</td>\n",
       "      <td>-0.143102</td>\n",
       "      <td>0.020306</td>\n",
       "      <td>-0.139674</td>\n",
       "      <td>0.052156</td>\n",
       "      <td>-0.408132</td>\n",
       "      <td>0.139542</td>\n",
       "      <td>0.122431</td>\n",
       "      <td>0.164689</td>\n",
       "      <td>0.138649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94699</th>\n",
       "      <td>__label__world</td>\n",
       "      <td>-0.038814</td>\n",
       "      <td>-0.109869</td>\n",
       "      <td>0.016513</td>\n",
       "      <td>0.057183</td>\n",
       "      <td>-0.339918</td>\n",
       "      <td>0.145005</td>\n",
       "      <td>-0.015179</td>\n",
       "      <td>0.134849</td>\n",
       "      <td>-0.202327</td>\n",
       "      <td>-0.179774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94700</th>\n",
       "      <td>__label__sports</td>\n",
       "      <td>0.295794</td>\n",
       "      <td>0.340246</td>\n",
       "      <td>-0.038027</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>-0.011269</td>\n",
       "      <td>0.242878</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>-0.299439</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>-0.252484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94701</th>\n",
       "      <td>__label__scitech</td>\n",
       "      <td>0.014180</td>\n",
       "      <td>-0.024344</td>\n",
       "      <td>0.007195</td>\n",
       "      <td>0.022572</td>\n",
       "      <td>0.277725</td>\n",
       "      <td>-0.010695</td>\n",
       "      <td>-0.159538</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>-0.309716</td>\n",
       "      <td>0.320987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94702 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5   \\\n",
       "0                      . -0.001581 -0.055738 -0.001461  0.013572 -0.024389   \n",
       "1                    the  0.041248  0.020253 -0.005631 -0.013228 -0.002068   \n",
       "2                      , -0.044975  0.007411 -0.001072 -0.001351  0.026816   \n",
       "3                        -0.091575 -0.034052  0.025836 -0.002135 -0.019016   \n",
       "4                     to  0.017022  0.029204 -0.007912  0.016093 -0.007380   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       "94697          maleafter  0.010264 -0.013049 -0.005277  0.017525 -0.015361   \n",
       "94698  __label__business -0.216275 -0.143102  0.020306 -0.139674  0.052156   \n",
       "94699     __label__world -0.038814 -0.109869  0.016513  0.057183 -0.339918   \n",
       "94700    __label__sports  0.295794  0.340246 -0.038027  0.061239 -0.011269   \n",
       "94701   __label__scitech  0.014180 -0.024344  0.007195  0.022572  0.277725   \n",
       "\n",
       "             6         7         8         9         10  \n",
       "0      0.012898 -0.027400  0.030329 -0.078572 -0.081473  \n",
       "1      0.004240 -0.013099  0.036625  0.028696  0.005871  \n",
       "2      0.001681  0.010960 -0.018680 -0.026508 -0.018127  \n",
       "3      0.052091 -0.035150 -0.017636 -0.067598  0.067879  \n",
       "4     -0.014567  0.000096  0.024154 -0.013684  0.001100  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "94697  0.006922 -0.019601 -0.002084 -0.017456  0.004337  \n",
       "94698 -0.408132  0.139542  0.122431  0.164689  0.138649  \n",
       "94699  0.145005 -0.015179  0.134849 -0.202327 -0.179774  \n",
       "94700  0.242878  0.025850 -0.299439  0.355422 -0.252484  \n",
       "94701 -0.010695 -0.159538  0.022877 -0.309716  0.320987  \n",
       "\n",
       "[94702 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/ag_news_csv/model.tsv', sep='\\t', header=None, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wen compute predictions and measure the peformance. In the test mode, StarSpace reports the hit@k evaluation metric which tells us how many correct answers are among the top k predictions. We are interested in the most probable category, therefore we use the hit@1 metric (in general, assignment of categories to text can be viewed as a multi-label classification problem). StarSpace achieves the score $hit@1=0.46$ which means that in 46% of test cases the model's first prediction is the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 50\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to load a trained starspace model.\n",
      "STARSPACE-2018-2\n",
      "Model loaded.\n",
      "Loading data from file : data/ag_news_csv/test.pp\n",
      "Total number of examples loaded : 7600\n",
      "Predictions use 4 known labels.\n",
      "------Loaded model args:\n",
      "Arguments: \n",
      "lr: 0.01\n",
      "dim: 10\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: dot\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 5\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 1\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 0\n",
      "fileFormat: fastText\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Predictions use 4 known labels.\n",
      "Evaluation Metrics : \n",
      "hit@1: 0.464737 hit@10: 1 hit@20: 1 hit@50: 1 mean ranks : 1.70079 Total examples : 7600\n"
     ]
    }
   ],
   "source": [
    "!./StarSpace/starspace test \\\n",
    "  -model \"data/ag_news_csv/model\" \\\n",
    "  -testFile \"data/ag_news_csv/test.pp\" \\\n",
    "  -ngrams 1 \\\n",
    "  -dim 10 \\\n",
    "  -label \"__label__\" \\\n",
    "  -thread 10 \\\n",
    "  -similarity \"dot\" \\\n",
    "  -trainMode 0 \\\n",
    "  -verbose false \\\n",
    "  -predictionFile \"data/ag_news_csv/test.y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result was obtained using the parameters as specified by the authors in the [published example](https://github.com/facebookresearch/Starspace/blob/master/examples/classification_ag_news.sh). The performance (46.4%) differs significantly from the published results [[1]](#fn1) where the authors report 91.6% accuracy on the test set for this task using the same number of dimensions (10).\n",
    "\n",
    "On the other hand, our implementation of the baseline classifier based on TF-IDF + SVM presented below shows similar performance (91%) to the BOW + multinomial logistic regression (88.8%) reported in the paper [[3]](#fn3).\n",
    "\n",
    "---\n",
    "<span id=\"fn3\"> [3]  Zhang, X., and LeCun, Y. 2015. Text understanding from scratch. arXiv preprint arXiv:1502.01710. </span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def to_tfidf(documents, dic=None, tfidf_model=None):\n",
    "    documents = [gensim.parsing.preprocessing.preprocess_string(doc) for doc in documents]\n",
    "    if dic is None:\n",
    "        dic = gensim.corpora.Dictionary(documents)\n",
    "        dic.filter_extremes()\n",
    "    bows = [dic.doc2bow(doc) for doc in documents]\n",
    "    if tfidf_model is None:\n",
    "        tfidf_model = gensim.models.tfidfmodel.TfidfModel(dictionary=dic)\n",
    "    tfidf_vectors = tfidf_model[bows]\n",
    "    return tfidf_vectors, dic, tfidf_model\n",
    "\n",
    "\n",
    "train = pd.read_csv('data/ag_news_csv/train.csv', names=['category', 'title', 'body'])\n",
    "X_train = [x.title + ' ' + x.body for x in train.itertuples()]\n",
    "y_train = [x.category for x in train.itertuples()]\n",
    "\n",
    "test = pd.read_csv('data/ag_news_csv/test.csv', names=['category', 'title', 'body'])\n",
    "X_test = [x.title + ' ' + x.body for x in test.itertuples()]\n",
    "y_test = [x.category for x in test.itertuples()]\n",
    "\n",
    "X_train_tfidf, dic, tfidf_model = to_tfidf(X_train)\n",
    "X_test_tfidf, _, __ = to_tfidf(X_test, dic, tfidf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF weighting used with the linear SVM achieves the accuracy of 91%. Because this is a multiclass classification problem, this metric is the same as hit@1, reported by StarSpace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.910\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(gensim.matutils.corpus2csc(X_train_tfidf, num_terms=len(dic)).T, le.transform(y_train))\n",
    "y_predicted = svc.predict(gensim.matutils.corpus2csc(X_test_tfidf, num_terms=len(dic)).T)\n",
    "print('Accuracy: {:.3f}'.format(metrics.accuracy_score(le.transform(y_test), y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have embeddings for a large number of words, so we can run clustering to see if the embeddings vectors can be used to partition words into four categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = pd.read_csv('data/ag_news_csv/model.tsv', sep='\\t', header=None, keep_default_na=False)\n",
    "embeddings = model[model.columns[1:]]\n",
    "kmeans = KMeans(n_clusters=4, random_state=12345).fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three smaller clusters closely match the topics Business, World, and Sci/Tech while the largest cluster is less specific and contains words from all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 (1640 instances)\n",
      "['us' 'company' 'oil' 'inc' 'yesterday' '?' 'corp' 'prices' 'years'\n",
      " 'group' 'season' 'deal' 'sales' 'business' 'billion' 'former'\n",
      " 'washington' 'profit' 'states' '/b&gt' 'b&gt' 'chief' 'american' 'shares'\n",
      " 'take' 'bank' 'third' 'federal' 'companies' 'co' 'maker' 'bid' 'largest'\n",
      " 'industry' 'big' 'giant' '5' 'growth' 'investor' '//www' 'href=http'\n",
      " '/a&gt' 'trade' 'earnings' 'dollar' 'buy' 'gold' 'union' 'amp' 'stock'\n",
      " 'loss' 'agreed' 'months' 'aspx' 'com/fullquote'\n",
      " 'target=/stocks/quickinfo/fullquote&gt' 'like' 'firm' 'air' 'rose'\n",
      " 'executive' 'jobs' 'update' 'price' 'boston' 'economy' 'drug' 'ahead'\n",
      " 'pay' 'near' 'biggest' 'economic' 'peoplesoft' 'car' 'o' 'street' 'work'\n",
      " 'your' 'free' '2005' 'much' '6' 'presidential' 'workers' 'wins' 'america'\n",
      " 'nation' 'share' 'financial' 'fall' 'wall' 'fell' 'lower' 'september'\n",
      " 'crude' 'october' 'chicago' 'job' '11' 'consumer']\n",
      "\n",
      "Cluster 1 (89619 instances)\n",
      "['the' ',' 'to' 'a' 'of' 'in' 'and' 's' 'on' 'for' '#39' ')' 'that' 'with'\n",
      " 'as' 'at' 'is' 'its' 'new' 'it' 'said' 'has' 'from' 'an' 'his' 'will'\n",
      " 'after' 'was' 'be' 'over' 'have' 'their' 'are' 'up' 'quot' 'but' 'more'\n",
      " 'first' 'two' 'he' 'world' 'this' '--' 'monday' 'wednesday' 'tuesday'\n",
      " 'out' 'thursday' 'one' 'not' 'against' 'friday' 'into' 'they' 'about'\n",
      " 'last' 'year' 'than' 'who' 'no' 'were' 'been' 'million' 'week' 'had'\n",
      " 'united' 'when' 'could' 'three' 'today' 'time' 'may' 'percent' '1' 'off'\n",
      " 'team' 'next' 'back' 'saturday' 'or' 'can' 'some' 'second' 'state' 'all'\n",
      " 'top' 'day' 'down' 'n' 'international' 'most' 'record' 'victory'\n",
      " 'officials' 'report' 'open' 'end' 'plans' 'court' 'if']\n",
      "\n",
      "Cluster 2 (1594 instances)\n",
      "['.' '-' \"'\" 'iraq' 'york' 'president' 'says' 'sunday' 'would'\n",
      " 'government' 'people' 'which' 'afp' 'win' 'night' 'china' 'minister'\n",
      " 'bush' 'killed' 'city' 'stocks' 'european' 'talks' 'league' 'country'\n",
      " 'reported' 'british' 'japan' 'india' 'police' 'prime' 'iraqi' 'leader'\n",
      " 'hit' 'say' 'baghdad' 'expected' 'election' 'north' 'under' 'war'\n",
      " 'australia' 'military' 'cut' 'nuclear' 'higher' 'un' 'official'\n",
      " 'palestinian' 'sox' 'attack' 'troops' 'russia' 'israeli' 'gaza' 'press'\n",
      " 'west' 'including' 'general' 'man' 'iran' 'football' 'forces' 'athens'\n",
      " 'past' 'europe' 'investors' 'peace' 'canadian' 'six' 'russian' 'beat'\n",
      " 'pakistan' 'held' 'public' 'eu' 'where' 'foreign' 'bomb' 'attacks'\n",
      " 'israel' 'nations' 'championship' 'korea' 'australian' 'kerry' 'leaders'\n",
      " 'french' 'men' 'house' 'death' 'killing' 'darfur' 'leading' 'arafat'\n",
      " 'capital' 'army' 'japanese' 'campaign' 'trial']\n",
      "\n",
      "Cluster 3 (1849 instances)\n",
      "['' '(' 'by' 'reuters' 'ap' '&lt' 'u' 'microsoft' 't' 'game' 'security'\n",
      " 'software' 'internet' '2' 'market' 'announced' 'news' '2004' 'service'\n",
      " 'you' 'before' 'technology' 'com' 'search' 'computer' 'space' 'online'\n",
      " 'what' 'network' 'google' 'ibm' 'research' 'according' 'music' 'help'\n",
      " 'while' 'games' 'web' 'san' 'mobile' 'services' '4' 'quarter' 'wireless'\n",
      " 'system' 'data' 'i' 'phone' 'apple' 'oracle' 'windows' 'global' 'intel'\n",
      " 'found' 'users' 'reports' 'released' 'release' 'offer' 'case' 'use' 'uk'\n",
      " 'video' 'pc' 'systems' 'support' 'nasa' 'sun' 'launch' 'linux' 'called'\n",
      " 'digital' 'scientists' 'net' 'program' 'version' 'future' 'center' 'site'\n",
      " 'customers' 'study' 'chip' 'sony' 'management' 'california' 'such'\n",
      " 'making' 'department' 'using' 'grand' 'ceo' 'university' 'tv' 'launched'\n",
      " 'times' 'source' 'server' 'better' 'phones' 'desktop']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_array = model[0].to_numpy()\n",
    "for ci in range(kmeans.n_clusters):\n",
    "    cluster_words = np.compress(kmeans.labels_==ci, words_array)\n",
    "    print('Cluster {} ({} instances)'.format(ci, len(cluster_words)))\n",
    "    print(cluster_words[:100])\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
