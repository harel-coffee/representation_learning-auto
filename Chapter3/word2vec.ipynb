{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "\n",
    "Word2vec models can be easily trained from freely available corpora on the Web. The training is straightforward when using libraries such as Gensim or the [original Google's implementation](https://github.com/tmikolov/word2vec) but may require a lot of time and computing resources. Luckily, many pre-trained models for different languages are available. The [NLPL word embeddings repository](http://vectors.nlpl.eu/repository/) contains models for different languages [[1]](#fn1). We will use the English model built on the English CoNLL17 corpus using the skipgram method, which has 4,027,169 words in the vocabulary, and 100 dimensions.\n",
    "\n",
    "The following libraries are required to run this notebook:\n",
    "\n",
    "- gensim==3.8.3\n",
    "- scikit-learn==0.23.2\n",
    "- matplotlib==3.3.2\n",
    "\n",
    "---\n",
    "<span id=\"fn1\"> [1] Fares, Murhaf; Kutuzov, Andrei; Oepen, Stephan & Velldal, Erik (2017). Word vectors, reuse, and replicability: Towards a community repository of large-text resources, In Jörg Tiedemann (ed.), Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017. Linköping University Electronic Press. ISBN 978-91-7685-601-7 </span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim uses the `KeyedVectors` structure which captures similarities of the results for methods such as Word2vec and FastText because the trained vectors have the same structure regardless of the training method. Using the `KeyedVectors` class we will load models trained with the original Google's word2vec implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, some inital configuration is required. We will use inline plotting of images in the png format, and we will ignore future warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to download the model and unpack the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "# download\n",
    "data = urlopen('http://vectors.nlpl.eu/repository/20/40.zip')\n",
    "with open('data/40.zip', \"wb\") as fp:\n",
    "    fp.write(data.read())\n",
    "# unzip data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('data/40.zip') as zf:\n",
    "    zf.extractall('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be loaded into a `KeyedVectors` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format('data/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model loaded we can get the vectors for any word in the vocabulary. However, out-of-vocabulary words are not handled automatically and the second example below demonstrates the exception returned when such a word is encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.022 -0.286 -0.204 -0.601  0.419 -0.295  0.122 -0.459  0.28   0.485\n",
      " -0.32   0.187 -0.246 -0.051 -0.308  0.499  0.365 -0.085  0.28   0.262\n",
      "  0.086  0.003  0.214 -0.808  0.074  0.125  0.072 -0.221  0.204  0.541\n",
      " -0.15   0.548  0.287  0.553 -0.663  0.276  0.064 -0.17  -0.164 -0.077\n",
      " -0.506  0.321 -0.077 -0.138  0.226  0.532  0.79   0.084  0.001 -0.204\n",
      "  0.028  0.174  0.059 -0.231 -0.009 -0.152 -0.14   0.078 -0.477 -0.419\n",
      " -0.604 -0.002 -0.339 -0.314 -0.403  0.317  0.249 -0.234  0.28  -0.023\n",
      "  0.155 -0.408  0.287 -0.034  0.28   0.137  0.43   0.32   0.129  0.017\n",
      "  0.169  0.013  0.174 -0.233 -0.145 -0.001 -0.112  0.096  0.197  0.023\n",
      "  0.051  0.137 -0.439 -0.281  0.298 -0.011 -0.542  0.551 -0.238  0.084]\n",
      "\"word '<html>' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(w2v.get_vector('dog'))\n",
    "try:\n",
    "    print(w2v.get_vector('<html>'))\n",
    "except KeyError as e:\n",
    "    print(e)\n",
    "    # use traceback.print_exc() for a detailed output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim offers several useful functions for exploiting the most interesting features of word embeddings. Let's demonstrate some of the word similarity tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.doesnt_match(['cat', 'dog', 'lion', 'snake', 'table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 0.8616693019866943),\n",
       " ('dogs', 0.8479607105255127),\n",
       " ('eskie', 0.8324236869812012),\n",
       " ('dachsund', 0.8204894065856934),\n",
       " ('doggie', 0.8190792798995972)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_word('dog', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet repeats the famous computation $\\text{king}-\\text{man}+\\text{woman}=\\text{queen}$ using Gensim's helper functions. We get the expected result as the highest ranked answer but other less relevant answers are also ranked high:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8596236705780029),\n",
       " ('king', 0.8447158336639404),\n",
       " ('debsirindra', 0.7552260756492615),\n",
       " ('empress', 0.7545188665390015),\n",
       " ('supayalat', 0.748792290687561)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similar_by_vector(w2v.get_vector('king') - w2v.get_vector('man') + w2v.get_vector('woman'), topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will use word2vec word vectors to compute document vectors by simply summing individual word vectors and multiplying each word vector with the corresponding _idf_ value. Such approach is fast and known to work well for word embeddings ([see the work of Zhao, Lan, and Tian](https://www.aclweb.org/anthology/S15-2021.pdf)). We will work with a tiny corpus of 9 titles of scientific papers. First, we need some very basic preprocessing which includes transformation to lowercase, tokenization based on white space characters, and filtering out any tokens that are not in our word2vec model vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs = [\n",
    "'New Insights Towards Developing Recommender Systems',\n",
    "'3D Convolutional Neural Networks for Dynamic Sign Language Recognition',\n",
    "'A DDoS Detection Method for Socially Aware Networking Based on Forecasting Fusion Feature Sequence',\n",
    "'An image J plugin for the high throughput image analysis of in vitro scratch wound healing assays',\n",
    "'Interactions between functionalised silica nanoparticles and Pseudomonas fluorescens biofilm matrix',\n",
    "'A multi-parametric screening platform for photosynthetic trait characterization of microalgae and cyanobacteria under inorganic carbon limitation',\n",
    "'How schools can reopen safely during the pandemic',\n",
    "'Carbon dioxide loss from tropical soils increases on warming',\n",
    "'Horse eyeballs and bone hammers surprising lives of the Neanderthals']\n",
    "docs = [x.lower().split() for x in raw_docs]\n",
    "docs = [[token for token in token_list if token in w2v.vocab] for token_list in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary and compute smoothed _idf_ using formula $\\text{idf} = \\log(\\frac{N}{n_t+1})$ (smoothing is recommended to avoid division by zero). Although _idf_ vector weighting will not have much effect in our case with only 9 documents, this is recommended in a general case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dic = gensim.corpora.Dictionary(docs)\n",
    "N = len(docs)\n",
    "idfs = {}\n",
    "for token in dic.token2id:\n",
    "    idfs[token] = np.log2(N / (dic.dfs[dic.token2id[token]]+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute document vectors by summing word vectors and weighting them using _idf_. The result is a matrix where each row is a dense document vector of the same dimension as the word2vec model (100 in our case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 100)\n"
     ]
    }
   ],
   "source": [
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    doc_vectors.append(np.sum([w2v.get_vector(token)*idfs[token] for token in doc], axis=0))\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "print(doc_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute a similarity matrix to see which documents are similar and how much. We can use the `cosine_similarity` function from scikit-learn and `matshow` from matplotlib for a visual representation of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.759 0.839 0.708 0.553 0.703 0.64  0.578 0.57 ]\n",
      " [0.759 1.    0.831 0.775 0.61  0.69  0.53  0.473 0.545]\n",
      " [0.839 0.831 1.    0.819 0.645 0.771 0.696 0.633 0.618]\n",
      " [0.708 0.775 0.819 1.    0.757 0.844 0.687 0.693 0.693]\n",
      " [0.553 0.61  0.645 0.757 1.    0.874 0.444 0.686 0.477]\n",
      " [0.703 0.69  0.771 0.844 0.874 1.    0.598 0.817 0.612]\n",
      " [0.64  0.53  0.696 0.687 0.444 0.598 1.    0.646 0.679]\n",
      " [0.578 0.473 0.633 0.693 0.686 0.817 0.646 1.    0.632]\n",
      " [0.57  0.545 0.618 0.693 0.477 0.612 0.679 0.632 1.   ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "sims = metrics.pairwise.cosine_similarity(doc_vectors)\n",
    "print(sims)\n",
    "_ = plt.matshow(sims, cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We display the least and the most similar documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least similar:\n",
      "Interactions between functionalised silica nanoparticles and Pseudomonas fluorescens biofilm matrix\n",
      "How schools can reopen safely during the pandemic\n",
      "\n",
      "Most similar:\n",
      "Interactions between functionalised silica nanoparticles and Pseudomonas fluorescens biofilm matrix\n",
      "A multi-parametric screening platform for photosynthetic trait characterization of microalgae and cyanobacteria under inorganic carbon limitation\n"
     ]
    }
   ],
   "source": [
    "# set the diagonal (self-similarity) to NaN so it can be ignored\n",
    "np.fill_diagonal(sims, np.nan)   \n",
    "imin = np.unravel_index(np.nanargmin(sims),sims.shape)\n",
    "imax = np.unravel_index(np.nanargmax(sims),sims.shape)\n",
    "print('Least similar:\\n{}\\n{}\\n'.format(raw_docs[imin[0]], raw_docs[imin[1]]))\n",
    "print('Most similar:\\n{}\\n{}'.format(raw_docs[imax[0]], raw_docs[imax[1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
