{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis\n",
    "\n",
    "We will demonstrate LSA on a dataset available in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The [Reuters-21578 Text Categorization Collection Data Set](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection) contains a collection of documents that appeared on Reuters newswire in 1987. It is one of the widely used document collections for text categorization <cite data-cite=\"reuters21578\"></cite>.\n",
    "\n",
    "According to the description, the data is stored in 22 files in the SGML format, which is a superset of HTML and XML. Using the `BeautifulSoup4` package we will extract the data from SGML files. Then, `NLTK` and `Gensim` will be used for text processing. The following Python libraries are needed to run this notebook:\n",
    "\n",
    "- gensim==3.8.3\n",
    "- nltk==3.5\n",
    "- beautifulsoup4==4.9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by downloading the archive and extracting `.sgm` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import tarfile\n",
    "\n",
    "# download\n",
    "data = urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz')\n",
    "with open('data/reuters21578.tar.gz', \"wb\") as fp:\n",
    "    fp.write(data.read())\n",
    "# read\n",
    "files = []\n",
    "tar = tarfile.open('data/reuters21578.tar.gz', 'r:gz')\n",
    "for member in tar.getmembers():\n",
    "    if member.name.endswith('.sgm'):\n",
    "        files.append(tar.extractfile(member))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the file objects now stored in `files`, we can proceed to extract the texts from SGML. We will configure the BeautifulSoup library to use the default HTML parser.\n",
    "\n",
    "The SGML files are structured in such a way that each document is inside a `<TEXT>` element which contains sub-elements such as `<TITLE>`, `<AUTHOR>`, `<BODY>` etc. We will extract the `<TITLE>` and `<BODY>` elements for articles that do not have the `TYPE=\"BRIEF\"` or `TYPE=\"UNPROC\"` as these two contain only the title or are unusual in some fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19043\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "documents = []\n",
    "for f in files:\n",
    "    sgml = BeautifulSoup(f, 'html.parser')\n",
    "    texts = sgml.findAll('text')\n",
    "    # filter types\n",
    "    texts = [x for x in texts if x.attrs.get('type') not in ['BRIEF', 'UNPROC']]\n",
    "    # find title and body and extract text\n",
    "    texts = [(x.find('title').get_text(), x.find('body').get_text()) for x in texts]\n",
    "    # end the title with a full stop and add the body\n",
    "    texts = ['{}. {}'.format(title, body) for title, body in texts]\n",
    "    # strip start-of-text and end-of-text ASCII characters and replace newlines with spaces\n",
    "    texts = [x.strip('\\x02\\x03').replace('\\n', ' ') for x in texts]\n",
    "    documents.extend(texts)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of the first 300 characters of the first document gives this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAHIA COCOA REVIEW. Showers continued throughout the week in the Bahia cocoa zone, alleviating the drought since early January and improving prospects for the coming temporao, although normal humidity levels have not been restored, Comissaria Smith said in its weekly review.     The dry period means\n"
     ]
    }
   ],
   "source": [
    "print(documents[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to perform some basic text preprocessing. First, three NLTK packages need to be downloaded. Then, the documents will be tokenized using the NLTK's `word_tokenize` function which internally performs sentence splitting followed by tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "tokenized_documents = [nltk.tokenize.word_tokenize(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lemmatization we will convert each word into its base form. For this task we will use the NLTK's Wordnet lemmatizer and provide part-of-speech (POS) tag of each word for better accuracy. The Wordnet lemmatizer uses Wordnet tags so we have to map POS tags to Wordnet tags first. Adjectives, verbs and adverbs are treated separately while everything else will be lemmatized as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# mapping from POS tags to Wordnet tags\n",
    "tag_map = collections.defaultdict(lambda: nltk.corpus.wordnet.NOUN)\n",
    "tag_map['J'] = nltk.corpus.wordnet.ADJ\n",
    "tag_map['V'] = nltk.corpus.wordnet.VERB\n",
    "tag_map['R'] = nltk.corpus.wordnet.ADV\n",
    "\n",
    "lemmatized_documents = []\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "for doc_tokens in tokenized_documents:\n",
    "    lemmatized_doc_tokens = [lemmatizer.lemmatize(token.lower(), tag_map[tag[0]]) for token, tag in nltk.pos_tag(doc_tokens)]\n",
    "    lemmatized_documents.append(lemmatized_doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection of the first 50 tokens of the lemmatized first documents gives this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bahia', 'cocoa', 'review', '.', 'shower', 'continue', 'throughout', 'the', 'week', 'in', 'the', 'bahia', 'cocoa', 'zone', ',', 'alleviate', 'the', 'drought', 'since', 'early', 'january', 'and', 'improve', 'prospect', 'for', 'the', 'come', 'temporao', ',', 'although', 'normal', 'humidity', 'level', 'have', 'not', 'be', 'restore', ',', 'comissaria', 'smith', 'say', 'in', 'it', 'weekly', 'review', '.', 'the', 'dry', 'period', 'mean']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_documents[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average length of the documents returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158.90395420889567"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(doc) for doc in lemmatized_documents])/len(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to remove tokens that are either too short, too long, composed of non-alphabetic characters or are stopwords (words that carry little or no meaning). We will use a standard stopword list available in the Gensim library. We will not develop our own corpus-specific list because it was demonstrated that this offers relatively little utility when training topic models <cite data-cite=\"stopwordsTopicModels\"></cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "lemmatized_documents = [[token for token in doc_tokens if 2<=len(token)<=15 and token.isalpha()] for doc_tokens in lemmatized_documents]\n",
    "lemmatized_documents = [[token for token in doc_tokens if token not in gensim.parsing.preprocessing.STOPWORDS] for doc_tokens in lemmatized_documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filtering reduced the average length of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.39468571128499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(doc) for doc in lemmatized_documents])/len(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic preprocessing is now complete. We proceed with the construction of the term-document matrix, a sparse matrix which contains document vectors. Different local and global weighting functions can be applied to transform each cell to be a product of a local term weight and a global weight. This is useful because rare words are likely to be more important than common words which appear in almost all documents. Raw term counts and TF-IDF weighting are two of the most popular combinations. We will use the Gensim library to perform _tf-idf_ weighting prior to LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = gensim.corpora.Dictionary(lemmatized_documents)\n",
    "bows = [dic.doc2bow(doc) for doc in lemmatized_documents]\n",
    "tfidf_model = gensim.models.tfidfmodel.TfidfModel(dictionary=dic)\n",
    "tfidf_vectors = tfidf_model[bows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the computed tf-idf vectors we can proceed with LSA. Note that in information retrieval literature and computer programming libraries the terms LSA and LSI (latent semantic indexing) are often used interchangeably. However, LSI typically refers to the actual SVD decomposition and dimensionality reduction, while LSA denotes the application of the technique to a wider range of problems. The Gensim library implements this decomposition in its `LsiModel` class. The following snippet performs truncated SVD decomposition while keeping only `k=10` largest singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = gensim.models.LsiModel(corpus=tfidf_vectors, id2word=dic, num_topics=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the results of truncated SVD decomposition $X=U*S*V^T$: left singular vectors (U), singular values (S), and right singular vectors (V). The rows of the truncated matrix U are embeddings of words from the dictionary, while the columns of $V^T$ are embeddings of documents into the lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35177, 10)\n",
      "[[-0.0001 -0.0011  0.0001 ...  0.0008  0.     -0.0001]\n",
      " [-0.0019 -0.0161  0.0022 ... -0.0099  0.0074 -0.0018]\n",
      " [-0.001  -0.0114  0.0017 ...  0.0093  0.0036 -0.0031]\n",
      " ...\n",
      " [-0.     -0.0001 -0.     ... -0.     -0.0001 -0.    ]\n",
      " [-0.     -0.0001 -0.     ... -0.     -0.0001 -0.    ]\n",
      " [-0.     -0.0001 -0.     ... -0.     -0.0001 -0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# human readable output of numpy matrices\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(lsi_model.projection.u.shape)\n",
    "print(lsi_model.projection.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[26.138 17.003 13.266 11.915 10.412 10.096  9.71   9.231  9.042  8.748]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(lsi_model.projection.s.shape)\n",
    "print(lsi_model.projection.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix with right singular vectors can grow big with a large number of documents, therefore it is not stored with the model and we have to compute it explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19043, 10)\n",
      "[[-0.002 -0.008  0.001 ... -0.007  0.003 -0.   ]\n",
      " [-0.001 -0.007  0.001 ... -0.002  0.003 -0.001]\n",
      " [-0.001 -0.007  0.001 ...  0.011  0.004 -0.003]\n",
      " ...\n",
      " [-0.    -0.003  0.    ...  0.     0.001 -0.001]\n",
      " [-0.001 -0.003  0.001 ...  0.001 -0.001  0.   ]\n",
      " [-0.001 -0.006 -0.    ... -0.001 -0.003 -0.001]]\n"
     ]
    }
   ],
   "source": [
    "C = gensim.matutils.corpus2dense(lsi_model[tfidf_vectors], len(lsi_model.projection.s)).T / lsi_model.projection.s\n",
    "print(C.shape)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect how latent dimensions (left singular vectors or topics) are defined. We will use the `show_topics` functions which displays topics ordered by significance using the desired number of most important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '-0.432*\"ct\" + -0.375*\"vs\" + -0.371*\"net\"'),\n",
       " (1, '-0.241*\"billion\" + -0.236*\"pct\" + -0.220*\"bank\"'),\n",
       " (2, '0.471*\"loss\" + -0.371*\"ct\" + -0.338*\"qtly\"'),\n",
       " (3, '0.641*\"loss\" + -0.308*\"net\" + -0.243*\"mln\"'),\n",
       " (4, '0.363*\"billion\" + -0.337*\"share\" + -0.211*\"offering\"'),\n",
       " (5, '0.386*\"billion\" + -0.240*\"tonne\" + 0.199*\"pct\"'),\n",
       " (6, '-0.389*\"bank\" + -0.291*\"stg\" + 0.215*\"billion\"'),\n",
       " (7, '-0.346*\"tonne\" + 0.345*\"billion\" + -0.307*\"stg\"'),\n",
       " (8, '0.530*\"stg\" + -0.264*\"bond\" + 0.221*\"mln\"'),\n",
       " (9, '0.858*\"oper\" + -0.154*\"billion\" + 0.149*\"excludes\"')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.show_topics(num_words=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics may look cryptic at the first glance for several reasons. First and foremost, when using LSA, there is no straightforward way to interpret a topic defined in terms of linear combinations of words. Only in the most trivial and obvious cases can the positive and negative contributions of words be explained in a reasonable way. Second, the Reuters-21578 dataset contains many financial news and plenty of abbreviations like `ct`, `qtly`, etc. which are important when defining latent dimensions in financial domains but are hard to interpret without background knowledge about the data. Finally, our _apriori_ decision that we keep only the ten largest singular values (to keep things simple and outputs brief) is not appropriate for a dataset of this size and structure. Further experiments by the reader, using different parameters, could improve the results (typical settings in the literature range from 100 to a few thousand in case of hundreds of thousands of documents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet allocation\n",
    "\n",
    "Latent Dirichlet allocation is very similar to probabilistic LSA (pLSA) but the topic distribution is assumed to have a sparse Dirichlet prior. To demonstrate LDA, we will use the same Reuters-21578 dataset and the same preprocessing that was used for LSA. Assuming that we have the `lemmatized_documents`, `dic`, and `bows` variables still populated, we can use the Gensim's `LdaModel` class.\n",
    "\n",
    "We will train a LDA model with 10 latent topics. Next, we will inspects the topics and their most relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.017*\"dlrs\" + 0.013*\"reuter\" + 0.009*\"new\" + 0.008*\"tonne\"'),\n",
       " (1, '0.030*\"dlrs\" + 0.028*\"pct\" + 0.025*\"mln\" + 0.024*\"company\"'),\n",
       " (2, '0.036*\"bank\" + 0.021*\"mln\" + 0.020*\"dlrs\" + 0.018*\"debt\"'),\n",
       " (3, '0.048*\"oil\" + 0.019*\"pct\" + 0.016*\"share\" + 0.015*\"company\"'),\n",
       " (4, '0.026*\"price\" + 0.023*\"president\" + 0.016*\"reuter\" + 0.013*\"executive\"'),\n",
       " (5, '0.015*\"billion\" + 0.015*\"market\" + 0.015*\"pct\" + 0.013*\"rate\"'),\n",
       " (6, '0.015*\"bond\" + 0.013*\"reuter\" + 0.013*\"issue\" + 0.012*\"pct\"'),\n",
       " (7, '0.049*\"share\" + 0.033*\"stock\" + 0.032*\"company\" + 0.026*\"reuter\"'),\n",
       " (8, '0.019*\"trade\" + 0.014*\"ec\" + 0.013*\"tonne\" + 0.012*\"year\"'),\n",
       " (9, '0.130*\"mln\" + 0.084*\"ct\" + 0.078*\"net\" + 0.058*\"vs\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = gensim.models.LdaModel(bows, id2word=dic, num_topics=10)\n",
    "lda.show_topics(num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed topics are too general to get meaningful insights, so we will repeat the training with 100 topics and display 10 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15,\n",
       "  '0.056*\"texaco\" + 0.037*\"canadian\" + 0.036*\"canada\" + 0.029*\"philippine\"'),\n",
       " (44,\n",
       "  '0.038*\"american\" + 0.031*\"express\" + 0.029*\"company\" + 0.022*\"shearson\"'),\n",
       " (20, '0.026*\"brand\" + 0.024*\"port\" + 0.020*\"uganda\" + 0.019*\"dlrs\"'),\n",
       " (58,\n",
       "  '0.095*\"telephone\" + 0.080*\"british\" + 0.042*\"aerospace\" + 0.037*\"week\"'),\n",
       " (70, '0.098*\"pacific\" + 0.067*\"canadian\" + 0.048*\"airline\" + 0.045*\"air\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = gensim.models.LdaModel(bows, id2word=dic, num_topics=100)\n",
    "lda.show_topics(num_topics=5, num_words=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to get the most relevant topics for a given word to obtain its sparse vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(55, 0.016039087), (61, 0.091784805)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_term_topics(dic.token2id['reagan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of topics for the given document can be obtained (a sparse vector representation of the document). The following snippet displays the probability of the three strongest topics for the first document along with the five strongest topic words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39589494 ['tonne', 'export', 'mln', 'wheat', 'sugar']\n",
      "0.11983482 ['kong', 'hong', 'new', 'dlrs', 'philip']\n",
      "0.087170556 ['coffee', 'quota', 'producer', 'cocoa', 'consumer']\n"
     ]
    }
   ],
   "source": [
    "top_3 = sorted(lda.get_document_topics(bows[0]), key=lambda x: x[1], reverse=True)[:3]\n",
    "for topic_id, p in top_3:\n",
    "    print(p, [x[0] for x in lda.show_topic(topic_id)][:5])   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
