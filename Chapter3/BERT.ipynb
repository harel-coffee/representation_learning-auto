{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "BERT [[1]](#fn1) is a successful neural architectures which excells in solving a variety of natural language processing tasks. Training an entire model from scratch required lots of computing resources but luckily there is a variety of pre-trained BERT models available which can be fine-tuned to particular downstream language processing tasks. \n",
    "\n",
    "We use the `transformers` library and experiment with the provided pre-trained BERT models using the [IMDB Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). The following libraries are required to run this notebook:\n",
    "\n",
    "- transformers==3.5.1\n",
    "- torch==1.7.0\n",
    "- tensorflow==2.3.1\n",
    "- gensim==3.8.3\n",
    "- scikit-learn==0.23.2\n",
    "- matplotlib==3.3.2\n",
    "- tensorflow-datasets==3.2.1\n",
    "\n",
    "----\n",
    "<span id=\"fn1\"> [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\n",
    "training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171â€“4186, 2019. </span>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB dataset is available for download on the internet but it is also available in the `tensorflow-datasets` package. We use this version.  First, we load the supervised part of the dataset and convert  it from `tf.Tensor` into Python data structures. We use only the supervised part (i.e., reviews with ratings) which contains 25,000 samples in each training and testing set. Movie reviews are categorized as negative when its $score<=4$ and as positive when $score>=7$. We skip the reviews with neutral rating in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets\n",
    "train, test = tensorflow_datasets.load('imdb_reviews', split=['train', 'test'], as_supervised=True)\n",
    "train = [(text.decode('utf-8'), int(score)) for text, score in tensorflow_datasets.as_numpy(train)]\n",
    "test = [(text.decode('utf-8'), int(score)) for text, score in tensorflow_datasets.as_numpy(test)]\n",
    "X_train, y_train = [x[0] for x in train], [x[1] for x in train]\n",
    "X_test, y_test = [x[0] for x in test], [x[1] for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute a baseline to which we can compare BERT. A typical baseline in text mining is a _tf-idf_ representation using support vector machines for prediction. We keep _tf-idf_ computation simple and efficient using gensim helper functions. Testing data is stored separatly and is not used when computing the dictionary and _idf_ weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "def to_tfidf(documents, dic=None, tfidf_model=None):\n",
    "    documents = [gensim.parsing.preprocessing.preprocess_string(doc) for doc in documents]\n",
    "    if dic is None:\n",
    "        dic = gensim.corpora.Dictionary(documents)\n",
    "        dic.filter_extremes()\n",
    "    bows = [dic.doc2bow(doc) for doc in documents]\n",
    "    if tfidf_model is None:\n",
    "        tfidf_model = gensim.models.tfidfmodel.TfidfModel(dictionary=dic)\n",
    "    tfidf_vectors = tfidf_model[bows]\n",
    "    return tfidf_vectors, dic, tfidf_model\n",
    "\n",
    "X_train_tfidf, dic, tfidf_model = to_tfidf(X_train)\n",
    "X_test_tfidf, _, __ = to_tfidf(X_test, dic, tfidf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use gensim's _tf-idf_ sparse vectors in classifiers from the scikit library, we convert them to scipy sparse matrices and transpose them so that documents become rows of the matrix. Then we can train the SVM classifier and use it for prediction. Finally, we compute the baseline accuracy (which turns out to be very good). The authors of the dataset reported only 3% better accuracy (88.89%) but they used extensive text preprocessing and also used the unlabeled part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "svc = LinearSVC()\n",
    "svc.fit(gensim.matutils.corpus2csc(X_train_tfidf).T, y_train)\n",
    "y_predicted = svc.predict(gensim.matutils.corpus2csc(X_test_tfidf).T)\n",
    "print('Accuracy: {:.3f}'.format(metrics.accuracy_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on BERT. We first use BERT's subword tokenizer which encodes input tokens into integers, inserts special tokens, and computes the attention mask which marks whether the model should pay attention or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"This was an absolutely terrible movie. Don't be lured in by Christopher \"\n",
      " 'Walken or Michael Ironside. ')\n",
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[  101,  2023,  2001,  2019,  7078,  6659,  3185,  1012,  2123,  1005,\n",
      "          1056,  2022, 26673,  1999,  2011,  5696,  3328,  2368,  2030,  2745,\n",
      "          3707,  7363,  1012,   102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from pprint import pprint\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence = X_train[0][:100]\n",
    "tokenized = tokenizer(sentence, return_tensors='pt')  # return arrays as PyTorch tensors\n",
    "pprint(sentence, compact=True)\n",
    "pprint(tokenized, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual token strings instead of integer IDs we convert IDs to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'this', 'was', 'an', 'absolutely', 'terrible', 'movie', '.', 'don',\n",
      " \"'\", 't', 'be', 'lured', 'in', 'by', 'christopher', 'walk', '##en', 'or',\n",
      " 'michael', 'iron', '##side', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.convert_ids_to_tokens(tokenized['input_ids'].tolist()[0]), compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT can be fine-tuned to a variety of NLP tasks. One of the uses of BERT is feature extraction where different combinations of layers can be used to extract contextual word embeddings. Although this approach cannot compete with fine-tuned models in term of predictive performance, the extracted vectors can be for e.g., visualiyation or similarity computations. Contextual embeddings can be obtained by applying the model to the input obtained from the tokenizer. By default, the model will return only the sequence of hidden states at the output layer but other hidden states can be obtained by setting the parameter `output_hidden_states=True` which causes the model to return 12 hidden states of the `bert-base` model along with the input embedding. The shape of a hidden state is determined by the batch size (1), the number of tokens (24) and the type of BERT (`bert-base` has 768 neurons per layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "torch.Size([1, 24, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True, output_hidden_states=True)\n",
    "output = model(**tokenized)\n",
    "print(len(output.hidden_states))\n",
    "print(output.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the authors of BERT, in order to get the best contextual embedding of a token for the named entitiy recognition task, the last four layers shall be concatenated. The resulting embedding vectors can be used in the same way as, e.g., word2vec embeddings with the additional benefit that the context is taken into account. The snippet below concatenates last four layers and construct an 1D tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0292, -0.4178, -0.3146,  ...,  0.4475,  0.6696,  0.1820],\n",
      "       grad_fn=<CatBackward>)\n",
      "torch.Size([3072])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "token_id = 1 # word \"this\"\n",
    "layers = [output.hidden_states[-i][0][token_id] for i in [3,2,1,0]]\n",
    "embedding = torch.cat(layers)\n",
    "print(embedding)\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transformers` package provides a ready-to-user pipeline with a fine-tuned model for binary text classification called `sentiment-analysis` which is based on variant of BERT called DistilBERT which preserves 95% of BERT's performance while running 60% faster and with 40% less parameters. The version in the pipeline is fine tuned for sequence classification (sentiment of the text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertModel\n",
    "\n",
    "sa_classifier = pipeline('sentiment-analysis')\n",
    "type(sa_classifier.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the pipeline on the test set and compute the accuracy. The DistilBERT model used in the pipeline limits its input to 512 tokens which is too short for some documents. As the pipeline does not automatically truncate the input, we have to check and truncate the input manually. Since BERT-based methods use subword tokenization, there is no easy way to find where to cut the original text to obtain a token sequence of length 512. We therefore limit all inputs to conservative 1024 characters which shall always be less than 512 tokens. This also significantly speeds up the computation with, hopefully, little decrease of the quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "predicted_sentiment = []\n",
    "MAX_CHARS = 1024\n",
    "for i,doc in enumerate(X_test):\n",
    "    doc = doc[:MAX_CHARS]\n",
    "    prediction = sa_classifier(doc)[0]\n",
    "    decision = 1 if prediction['label'] == 'POSITIVE' else 0\n",
    "    predicted_sentiment.append(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the predicted sentiment, we can evaluate the quality of the produced BERT-based classifier. It turns out that it is already better than our baseline classifier and very close to the best result reported by the authors of the dataset. Taking into account that the model was trained on different data such a result is outstanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy: {:.3f}'.format(metrics.accuracy_score(y_test, predicted_sentiment)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fine-tune BERT on the actual IMDB dataset. Because the details of fine-tuning procedure are not within the scope of this notebook, we use the provided `bert_finetune_classification.py` script which provides functions for basic fine-tuning. We can experimet with a number of parameters, e.g., the number of epochs, maximal length of the input, batch size, etc. We use the default values wherever possible and set the maximal length of the input to 128 in order to speed up the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: GeForce RTX 2060 SUPER\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   300   |   0.375713   |     -      |     -     |  124.97  \n",
      "   1    |   600   |   0.307289   |     -      |     -     |  126.03  \n",
      "   1    |   781   |   0.288199   |     -      |     -     |   75.95  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   300   |   0.168120   |     -      |     -     |  127.00  \n",
      "   2    |   600   |   0.149514   |     -      |     -     |  126.97  \n",
      "   2    |   781   |   0.143595   |     -      |     -     |   76.32  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import bert_finetune_classification as bft\n",
    "bft.set_seed(123)\n",
    "data = {'X_train': X_train,\n",
    "        'X_val': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_test}\n",
    "inputs = bft.preprocess(data, max_len=128)\n",
    "loaders = bft.make_dataloaders(data, inputs)\n",
    "bert_classifier, optimizer, scheduler = bft.initialize_model(loaders, epochs=2)\n",
    "bft.train(bert_classifier, optimizer, scheduler, loaders, epochs=2, evaluation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the fine-tuned model we can predict the ratings for the reviews in the test set. The resulting probabilities are converted into 0/1 score using 0.5 as the threshold. The obtained accuracy of the fine-tuned model is better than the best reported. We believe that it can be further improved by optimizing the parameters of the model. Try for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted_probs = bft.bert_predict(bert_classifier, loaders['val_dataloader'])\n",
    "y_predicted = np.where(predicted_probs[:,0] >= 0.5, 0, 1)\n",
    "print('Accuracy: {:.3f}'.format(metrics.accuracy_score(y_test, y_predicted)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
